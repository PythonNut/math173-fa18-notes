%&myformat
\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{commath}
\usepackage{xfrac}
\usepackage{siunitx}
\usepackage[parfill]{parskip}
\usepackage[makeroom]{cancel}
\usepackage{mathtools}
\usepackage{tabu}
\usepackage{bm}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage{booktabs}
\usepackage{tikz-cd}
\usepackage{mathrsfs}
\newcommand{\ihat}{\mathbf{\hat i}}
\newcommand{\jhat}{\mathbf{\hat j}}
\newcommand{\khat}{\mathbf{\hat k}}
\newcommand{\rhat}{\mathbf{\hat r}}
\newcommand{\thetahat}{\bm{\hat \theta}}

\DeclareMathOperator{\trace}{tr}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\nullity}{nullity}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\vspan}{span}
\DeclareMathOperator{\curl}{curl}
\DeclareMathOperator{\divergence}{div}
\DeclareMathOperator{\hess}{H}
\DeclarePairedDelimiter\ip{\langle }{\rangle}

\newcommand{\m}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\sm}[1]{\left[\begin{smallmatrix} #1 \end{smallmatrix}\right]}
\newcommand{\vm}[1]{\begin{vmatrix} #1 \end{vmatrix}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\QQ}{\mathbb{Q}}

\setlength{\parskip}{\bigskipamount}

\renewcommand{\labelenumi}{{\arabic{enumi}.}}
\renewcommand{\labelenumii}{(\roman{enumii})}
\renewcommand{\labelenumiii}{(\arabic{enumiii})}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\usepackage{xparse}

\ExplSyntaxOn
    \NewDocumentEnvironment{aside} { o }
     {\noindent\ignorespaces\\[0.5em]
       \begin{mdframed}[nobreak=true,backgroundcolor=gray!10]
         \IfNoValueTF{#1}{}{\textbf{#1}}
     }
     {
     \end{mdframed}
     }
\ExplSyntaxOff

\begin{document}

\begin{center}
  \Large Advanced Linear Algebra Week 1 Day 1
  \normalsize

  \textbf{2018/09/10} -- Jonathan Hayase, updated by Prof. Weiqing Gu
\end{center}

\section{Important things about eigenvalues and eigenvectors}

Suppose we have \(A\bm x_i = \lambda \bm x_i\) for \(i = 1, 2, \dots, n\).
Then we have
\[\del{A\bm x_1, A\bm x_2, \dots, A\bm x_n } = \del{\lambda_1\bm x_1, \lambda_2 \bm x_2,\dots, \lambda_n\bm x_n}.\]
Vectorizing this yields:
\[A \underbrace{\del{\bm x_1, \dots, \bm x_n}}_{P}= \underbrace{\del{\bm x_1, \dots, \bm x_n}}_{P} \underbrace{\sm{\lambda_1&&\\&\ddots&\\&&\lambda_n}}_{D}\]
\[AP = PD \implies A = PDP^{-1}\]
This is useful because, when calculating high powers of \(A\) we get:
\begin{align*}
  A^k &= \del{PDP^{-1}}^k && \text{for \(k = 10,000\)}\\
     &= \underbrace{PD\cancel{P^{-1}P}DP^{-1}\cdots PDP^{-1}}_{\text{\(k\) times}}\\
     &= PD^kP^{-1}\\
     &= P \sm{\lambda_1^k&&\\&\ddots&\\&&\lambda_n^k}
\end{align*}

\section{Least squares example}

Suppose we have
\[y = a_0 + a_1\bm x_1 + \cdots + a_n \bm x_n\]
But really, we have
\begin{align*}
  y^{(i)} &= \theta _0 + \theta _1\bm x_1^{(1)} + \cdots + \theta _n \bm x_n^{(i)}\\
  & \vdots

  y^{(N)} &= \theta _0 + \theta _1\bm x_1^{(N)} + \cdots + \theta _n \bm x_n^{(N)}
\end{align*}
For \(n \ll N\).

We can write this in vectorized form as:
\[\sm{y^{(1)}\\\vdots\\y^{(N)}} = \underbrace{\sm{1 & x_1^{(1)} & \cdots & x_n^{(1)}\\\vdots & &  \ddots & \vdots\\ 1 & x_1^{(N)} & \cdots & x_n^{(N)}}}}_{\text{design matrix}} \sm{\theta_0\\\vdots\\\theta_n}\]

So then we have:
\[y = X\bm \theta\]
\[X^Ty = X^TX\bm \theta\implies \bm \theta = \del{X^TX}^{-1}\del{X^TX}\]
\begin{aside}
  \begin{align*}
    \del{X^TX}^T &= X^T\del{X^T}^T\\
    &= X^TX
  \end{align*}
  So \(X^TX\) is symmetric.
\end{aside}

Symmetric matrices are diagonalizable.


\begin{aside}[Recall]
  \(A_{n\times n}\) Symmetric
  \begin{enumerate}
  \item All the eigenvalues of \(A\) are real
  \item All eigenvectors belonging to distinct eigenvalues are orthogonal
  \end{enumerate}
\end{aside}

Suppose we have
\[A = \sm{2 & 1 & 1\\ 1 & 2 & 1\\ 1 & 1 & 2}\]
Then we have
\begin{align*}
  0 &= \det\del{A - \lambda I}\\
    &=\m{2 -\lambda& 1 & 1\\ 1 & 2 -\lambda& 1\\ 1 & 1 & 2-\lambda}\\
  &= \m{4 -\lambda& 4-\lambda & 4-\lambda\\ 1 & 2 -\lambda& 1\\ 1 & 1 & 2-\lambda}\\
    &= (4-\lambda)\m{1 & 1 & 1\\ 1 & 2 -\lambda& 1\\ 1 & 1 & 2-\lambda}\\
  &= (4-\lambda)\m{1 & 1 & 1\\ 0 & 1 -\lambda& 0\\ 0 & 0 & 1-\lambda}\\
                                                           &=
\end{align*}

Suppose we have a bunch of vectors, which are not orthonormal.
\[Av_1 = \lambda v_1\]
First, we normalize \(v_1\) to get \(u_1 = \frac{v_1}{\norm{v_1}}\).
\[A\frac{v_1}{\norm{v_1}} = \lambda \frac{v_1}{\norm{v_1}}\]
\[v_2 \cdot u_1 = \norm{v_2} \cancelto{1}{\norm{u_1}} \cos \theta\]

Gram-Schmidt leads to the QR facotrization.

\section{Orthonormal Eigenbases}

Suppose we have an orthonormal eigenbasis

Then \(P  = \del{\bm u_1, \dots, \bm u_n}\) and we have \(P^TP = I\), so \(P^{-1} = P^T\) and
\[P^{-1}AP = D \implies P^TAP = D \implies A = PDP^T\]
If \(A\) is positive-definite then \(A\) has all positive eigenvalues
\begin{align*}
  D &= \m{\lambda_1 &&\\&\ddots&\\&&\lambda_n} = \m{\sqrt{\lambda_1} &&\\&\ddots&\\&&\sqrt{\lambda_n}}\m{\sqrt{\lambda_1} &&\\&\ddots&\\&&\sqrt{\lambda_n}}\\
 A &= P\sqrt{D}\sqrt{D}P^T = \del{P\sqrt{D}P^T}\del{P\sqrt{D}P^T} = LL = L^2
\end{align*}


jburkert@headlandstech.com careers@headlandstech.com
\footnote{Something something pfaffian}
\section{Inner Products}
An inner-product is a positive bilinear form.
\[\RR \times \RR \to \RR\]
\[(v, w) \mapsto v \cdot w \text{ or \(\ip{v, w}\)}\]

Or if \(f, g \in C[a, b]\) then
\[\ip{f, g} = \int_a^bfg \,\dif x\]

Inner Products have the following properties
\begin{enumerate}
\item Bilinear
\item Symmetric
\item Positive Definite
\end{enumerate}

Inner Products also induce a norm like so:
Let \(v = w\) then \(\ip{v,v} = \norm{v}^2\) so \(\norm{v} = \sqrt{\ip{v, v}}\).
\footnote{Something something banach space}

\section{Vector Spaces of Infinite Dimension}
Suppose you are working in the field \(\CC\).

Notation: \(\sfrac{V}{\CC}\) is a vector space over \(\CC\)

\[\bm z = \sm{z_1\\\vdots\\ z_n}, \bm w = \sm{w_1\\\vdots\\ w_n}\]
Then we have \(\ip{z, w} =\bm z_1 \overline{\bm w_1} + \dots + \bm z_n \overline{\bm w_n} \)

Now suppose we have \(\sfrac{V}{\CC}\) of \(n = \infty\) dimensions.
We see that \(\ip{\bm z, \bm z} = \norm{z_1}^2 + \cdots + \norm{z_n}^2 \geq 0\).
And we have \(\norm{z} = 0\) iff \(\norm{z_k} = 0\) for \(k \in \set{1,2\dots, k}\).

Also we have:
\[\overline{\ip{\bm z, \bm w}} = \overline{\bm z_1 \overline{\bm w_1} + \dots + \bm z_n \overline{\bm w_n}} =\overline{ \bm z_1} \bm w_1 + \dots + \overline{\bm z_n} \bm w_n\]

\[\ip{c\bm z_1 + \bm z_2, \bm w} = c\ip{\bm z_1, \bm w} + \ip{\bm z_2, \bm w}\]

Definition:

Let \(X\) be a vector space over field \(K\).

A scalar (or inner) product on \(X\) is

\[\ip{\cdot, \cdot} : X \times X \to K\]
Satisfying
For any \(k \in K\) and \(x, y \in X\)
\begin{enumerate}
\item \(\ip{x, x} \geq 0\)
\item
\end{enumerate}

\section{Examples}
\[\ell^2\del{\RR} = \set{\set{x_i}_{i=0}^\infty \mid x_i \in \RR \land \sum x_i^2 \leq \infty}\]
\[\ip{\set{x_i}_{i=0}^\infty, \set{y_i}_{i=0}^\infty} \stackrel{?}{=} \sum_{i = 0}^\infty x_i y_i\]
Does this converge.
(Yes, but we will see this later.)

Now consider \(L^2\intcc{a, b} = \set{f \mid f : \intcc{a, b} \to \RR \land \int_a^b\del{f(x)}^2 \,\dif x \text{ exists and is finite}}\)
\[\ip{f, g} = \int_a^b fg\,\dif x\]

Now consider \(\ell^2\del{\CC} \set{\set{z_i}_{i = 0}^\infty \mid z_i \in \CC \land \sum \norm{z_i} < \infty}\)
And we define
\[\ip{\set{z_i}_{i=0}^\infty, \set{w_i}_{i=0}^\infty} = \sum_{i = 0}^\infty z_i \overline{w_i}\]
This also converges.

\section{Zorn's Lemma}
\begin{aside}[Recall]

  \textbf{Totally ordered:} \There is a relation \(R\) (representing \(\leq\)) on a set \(A\).
  \begin{enumerate}
  \item Antisymmetry: for all \(a, b \in A\) if \(a \leq b \land b \leq a \implies a = b\).
  \item Transitivity: For all \(a,b,c \in A\) if \(a \leq b \land b \leq c \implies a \leq c\)
  \item \(a \leq b\) or \(b \leq a\)
  \end{enumerate}
  (You may substitue \(x \leq y\) with \(R( x, y)\) in the above.)

  \textbf{Partially ordered:}
  Example: Let a set \(A = \set{a, b, c}\) now, consider the powerset \(\mathcal{P}(A) = \set{\emptyset, \set{a}, \set{b}, \set{c}, \set{a, b}, \set{a, b}, \set{a, c}, A}\).
  Inclusion ``\(\subseteq\)'' is a partial order on \(\mathcal{P}(A)\), but not a total order.

  You can make a Hasse Diagram representing this ordering
  \begin{center}
    \begin{tikzcd}
      &A\\
      \set{a, b} \ar[ru] & \set{a, c} \ar[u] & \set{ b, c} \ar[lu]\\
      \set{a} \ar[u] \ar[ru]& \set{b} \ar[lu] \ar[ru]& \set{c} \ar[u] \ar[lu]\\
      & \emptyset \ar[lu] \ar[u] \ar[ru]
    \end{tikzcd}
  \end{center}
  Draw an arrow from \(x\) to \(y\) if \(x \mathop{\mathrm{R}} y\) (i.e. \(x \subseteq y\)).
  We call a sequence \(\emptyset \subseteq \set{b} \subseteq \set{b, c} \subseteq A\) a chain.

  Note: we can't compare \(\set{b}\) and \(\set{c}\).
\end{aside}

\textbf{Zorn's Lemma}: Let \(P\) be a partially ordered set such that every chain has an upper bound in \(P\).
Then the set \(P\) contains at least one maximum element.

Idea of proof:

\begin{aside}[Recall]
  In Linear Algebra you will use three kinds of basic proving techniques:
  \begin{enumerate}
  \item Induction
  \item Contradiction

    Example: (Zorn's Lemma)
    Use Hasse diagram of \(\del{P, R}\).
    Suppose there were no such maximal element.
    Then a chain can go forever.
    There exists some chain in the Hasse diagram which has larger and larger elements, without bound.
    But then this contradicts the assertion that all chains are upper bounded.

  \item Build a bridge\footnote{You basically start from the definition (and maybe also your goal) and meet in the middle.}

    Example: Prove ``inverse of \(A_{n\times n}\) is unique.''
    Suppose \(AB =BA = I\) and \(AC = CA = I\) want to show  \(B = C\).
    Then we build the following bridge:
    \[B = BI = B\del{AC} = \del{BA}C = IC = C\]

  \end{enumerate}
\end{aside}

\section{Dual Space}
\subsection{Motivation}
Theorem: Let \(x_i \in \RR^n\) be a sparse vector.
Let \(A\) be a \(m \times n\) random matrix with \(A_{ij} \sim \mathcal{N}(0, \sfrac{1}{m})\).
Then \(x_0\) is the unique optimal solution minimizing \(\norm{x}_1\) subject to \(Ax = Ax_0\).

Changing point view:
Let \(A\) change, \(x_0\) is fixed.

\[m \geq (1 + \epsilon) 2S \log n + s + 1\]
With probability at least
\[1 - 2n^{-\sbr{\sqrt{\sfrac{1 + \epsilon}{2s} + \epsilon} - \sqrt{\sfrac{1 + \epsilon}{2s}}}^2}\]

\subsection{Definition}
\begin{aside}[Recall]
  If we have an orthonormal basis \(\set{u_1, \dots, u_n}\) of a vector space \(\sfrac{V}{\RR}\) if we have any vector \(\bm v = a_1 \bm x_n + \cdots + a_n \bm u_n\).
  \begin{align*}
    a_1 &= \bm v_1 \cdot \bm u_1\\
       &\hspace{0.5em}\vdots\\
    a_n &= \bm v_n \cdot \bm u_n
  \end{align*}
\end{aside}

\subsection{Linear Functional}

Let \(V\) be a vector space over a field \(\mathbb{F}\).
Then a linear maps \(\ell : V \to \mathbb{F}\) is also called a linear functional on \(V\).

\begin{enumerate}
\item Basic example: Define \(L : \mathbb{F}^n \to \mathbb{F}\) where
  \[\bm x = \m{\bm x_1\\\vdots\\\bm x_n} \mapsto a_1 \bm x_1 + \cdots + a_n \bm x_n = \m{a_1 & \cdots & a_n}\m{\bm x_1\\\vdots\\\bm x_n} = \ell(\bm x)\]
  If you just multiply by a row vector, it's called functional.

\item \(\trace A = \)
  \[\mathbb{F}^n \to \mathbb{F}\]
  \[\trace\del{cA + B} = c\trace A + \trace B\]

\item ``Evaluation at a point''
  is a linear function on \(P_n(\CC)\)
  \[L:P_n(\CC) \to \CC\]

\item ``Integration''
  \[C\intcc{a, b} \to \RR\]
  \[L(f) \to \int_a^b f(t)\,\dif t\]


\item ``Derivative''
  \[L:C^2 \to \RR\]
\end{enumerate}
\end{document}
